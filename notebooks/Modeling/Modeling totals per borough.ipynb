{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915333ad-5789-47c0-8bc0-8e511bb1445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "# To view the entire dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Optuna Logging\n",
    "import optuna.logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) # To disable displaying trial results in hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab97f28-03cb-481f-b5ca-1d4010ea3352",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c95999c-dd6b-4f5f-8dea-8d8885839787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txn_date</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>txn_hour</th>\n",
       "      <th>week_day</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_monday</th>\n",
       "      <th>is_friday</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_holiday_next_day</th>\n",
       "      <th>is_holiday_previous_day</th>\n",
       "      <th>is_long_weekend</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>is_business_hour</th>\n",
       "      <th>is_night_hour</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>num_txns_All</th>\n",
       "      <th>lag_24</th>\n",
       "      <th>lag_25</th>\n",
       "      <th>lag_26</th>\n",
       "      <th>lag_27</th>\n",
       "      <th>lag_28</th>\n",
       "      <th>lag_29</th>\n",
       "      <th>lag_30</th>\n",
       "      <th>lag_31</th>\n",
       "      <th>lag_32</th>\n",
       "      <th>lag_33</th>\n",
       "      <th>lag_34</th>\n",
       "      <th>lag_35</th>\n",
       "      <th>lag_36</th>\n",
       "      <th>lag_37</th>\n",
       "      <th>lag_38</th>\n",
       "      <th>lag_39</th>\n",
       "      <th>lag_40</th>\n",
       "      <th>lag_41</th>\n",
       "      <th>lag_42</th>\n",
       "      <th>lag_43</th>\n",
       "      <th>lag_44</th>\n",
       "      <th>lag_45</th>\n",
       "      <th>lag_46</th>\n",
       "      <th>lag_47</th>\n",
       "      <th>lag_48</th>\n",
       "      <th>lag_72</th>\n",
       "      <th>lag_96</th>\n",
       "      <th>lag_120</th>\n",
       "      <th>lag_144</th>\n",
       "      <th>lag_164</th>\n",
       "      <th>lag_165</th>\n",
       "      <th>lag_166</th>\n",
       "      <th>lag_167</th>\n",
       "      <th>lag_168</th>\n",
       "      <th>lag_169</th>\n",
       "      <th>lag_68</th>\n",
       "      <th>lag_69</th>\n",
       "      <th>lag_70</th>\n",
       "      <th>lag_71</th>\n",
       "      <th>lag_73</th>\n",
       "      <th>lag_139</th>\n",
       "      <th>lag_140</th>\n",
       "      <th>lag_141</th>\n",
       "      <th>lag_142</th>\n",
       "      <th>lag_143</th>\n",
       "      <th>lag_145</th>\n",
       "      <th>lag_336</th>\n",
       "      <th>lag_504</th>\n",
       "      <th>lag_672</th>\n",
       "      <th>lag_840</th>\n",
       "      <th>lag_1008</th>\n",
       "      <th>lag_1176</th>\n",
       "      <th>lag_1344</th>\n",
       "      <th>rolling_avg_24_48_72_lags</th>\n",
       "      <th>std_24_48_72_lags</th>\n",
       "      <th>rolling_avg_w1-w4_lags</th>\n",
       "      <th>std_w1-w4_lags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp_hour</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-02-01 00:00:00</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 01:00:00</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>1110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 02:00:00</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 03:00:00</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>669</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 04:00:00</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      txn_date  quarter  month  day_of_month  txn_hour  \\\n",
       "timestamp_hour                                                           \n",
       "2019-02-01 00:00:00 2019-02-01        1      2             1         0   \n",
       "2019-02-01 01:00:00 2019-02-01        1      2             1         1   \n",
       "2019-02-01 02:00:00 2019-02-01        1      2             1         2   \n",
       "2019-02-01 03:00:00 2019-02-01        1      2             1         3   \n",
       "2019-02-01 04:00:00 2019-02-01        1      2             1         4   \n",
       "\n",
       "                     week_day  is_weekend  is_monday  is_friday  is_holiday  \\\n",
       "timestamp_hour                                                                \n",
       "2019-02-01 00:00:00         5           1          0          0           0   \n",
       "2019-02-01 01:00:00         5           1          0          0           0   \n",
       "2019-02-01 02:00:00         5           1          0          0           0   \n",
       "2019-02-01 03:00:00         5           1          0          0           0   \n",
       "2019-02-01 04:00:00         5           1          0          0           0   \n",
       "\n",
       "                     is_holiday_next_day  is_holiday_previous_day  \\\n",
       "timestamp_hour                                                      \n",
       "2019-02-01 00:00:00                  0.0                      0.0   \n",
       "2019-02-01 01:00:00                  0.0                      0.0   \n",
       "2019-02-01 02:00:00                  0.0                      0.0   \n",
       "2019-02-01 03:00:00                  0.0                      0.0   \n",
       "2019-02-01 04:00:00                  0.0                      0.0   \n",
       "\n",
       "                     is_long_weekend  is_rush_hour  is_business_hour  \\\n",
       "timestamp_hour                                                         \n",
       "2019-02-01 00:00:00                0             0                 0   \n",
       "2019-02-01 01:00:00                0             0                 0   \n",
       "2019-02-01 02:00:00                0             0                 0   \n",
       "2019-02-01 03:00:00                0             0                 0   \n",
       "2019-02-01 04:00:00                0             0                 0   \n",
       "\n",
       "                     is_night_hour  sin_hour  cos_hour  num_txns_All  lag_24  \\\n",
       "timestamp_hour                                                                 \n",
       "2019-02-01 00:00:00              1  0.000000  1.000000          1901     NaN   \n",
       "2019-02-01 01:00:00              1  0.258819  0.965926          1110     NaN   \n",
       "2019-02-01 02:00:00              1  0.500000  0.866025           772     NaN   \n",
       "2019-02-01 03:00:00              1  0.707107  0.707107           669     NaN   \n",
       "2019-02-01 04:00:00              1  0.866025  0.500000           794     NaN   \n",
       "\n",
       "                     lag_25  lag_26  lag_27  lag_28  lag_29  lag_30  lag_31  \\\n",
       "timestamp_hour                                                                \n",
       "2019-02-01 00:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 01:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 02:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 03:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 04:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "                     lag_32  lag_33  lag_34  lag_35  lag_36  lag_37  lag_38  \\\n",
       "timestamp_hour                                                                \n",
       "2019-02-01 00:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 01:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 02:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 03:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 04:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "                     lag_39  lag_40  lag_41  lag_42  lag_43  lag_44  lag_45  \\\n",
       "timestamp_hour                                                                \n",
       "2019-02-01 00:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 01:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 02:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 03:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2019-02-01 04:00:00     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "                     lag_46  lag_47  lag_48  lag_72  lag_96  lag_120  lag_144  \\\n",
       "timestamp_hour                                                                  \n",
       "2019-02-01 00:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 01:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 02:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 03:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 04:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "\n",
       "                     lag_164  lag_165  lag_166  lag_167  lag_168  lag_169  \\\n",
       "timestamp_hour                                                              \n",
       "2019-02-01 00:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 01:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 02:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 03:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 04:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "                     lag_68  lag_69  lag_70  lag_71  lag_73  lag_139  lag_140  \\\n",
       "timestamp_hour                                                                  \n",
       "2019-02-01 00:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 01:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 02:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 03:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "2019-02-01 04:00:00     NaN     NaN     NaN     NaN     NaN      NaN      NaN   \n",
       "\n",
       "                     lag_141  lag_142  lag_143  lag_145  lag_336  lag_504  \\\n",
       "timestamp_hour                                                              \n",
       "2019-02-01 00:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 01:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 02:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 03:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2019-02-01 04:00:00      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "                     lag_672  lag_840  lag_1008  lag_1176  lag_1344  \\\n",
       "timestamp_hour                                                        \n",
       "2019-02-01 00:00:00      NaN      NaN       NaN       NaN       NaN   \n",
       "2019-02-01 01:00:00      NaN      NaN       NaN       NaN       NaN   \n",
       "2019-02-01 02:00:00      NaN      NaN       NaN       NaN       NaN   \n",
       "2019-02-01 03:00:00      NaN      NaN       NaN       NaN       NaN   \n",
       "2019-02-01 04:00:00      NaN      NaN       NaN       NaN       NaN   \n",
       "\n",
       "                     rolling_avg_24_48_72_lags  std_24_48_72_lags  \\\n",
       "timestamp_hour                                                      \n",
       "2019-02-01 00:00:00                        NaN                NaN   \n",
       "2019-02-01 01:00:00                        NaN                NaN   \n",
       "2019-02-01 02:00:00                        NaN                NaN   \n",
       "2019-02-01 03:00:00                        NaN                NaN   \n",
       "2019-02-01 04:00:00                        NaN                NaN   \n",
       "\n",
       "                     rolling_avg_w1-w4_lags  std_w1-w4_lags  \n",
       "timestamp_hour                                               \n",
       "2019-02-01 00:00:00                     NaN             NaN  \n",
       "2019-02-01 01:00:00                     NaN             NaN  \n",
       "2019-02-01 02:00:00                     NaN             NaN  \n",
       "2019-02-01 03:00:00                     NaN             NaN  \n",
       "2019-02-01 04:00:00                     NaN             NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "borough = \"Bronx\" # \"Manhattan\" \"Brooklyn\" \"Queens\" \"Staten Island\" \"Bronx\" \"EWR\"\n",
    "filename = rf\"../../data/with_feature_engineering_totaled/{borough} - all txns.parquet.gz\" #\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "# Get the target column\n",
    "target_col = \"num_txns_All\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22585be7-c2e5-4c10-9220-f137fc6e83e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Statistic: -12.258276\n",
      "p-value: 9.203157023524619e-23\n",
      "Critical Values:\n",
      "\t1%: -3.431\n",
      "\t5%: -2.862\n",
      "\t10%: -2.567\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "time_series = df[(df['txn_date'] >= '2023-01-01') & (df['txn_date'] <= '2024-12-01')][target_col].to_numpy()\n",
    "result = adfuller(time_series)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value:', result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1cb9d3-a18a-461d-abef-35028ebf3c67",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "\n",
    "We use January 2023 to July 2024 data as the training set and August 2024 as the test/holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34eb9ef-87a8-4b78-b8bf-26902e5efad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Training data\n",
    "df_train = (\n",
    "    df\n",
    "    [(df['txn_date'] >= '2023-01-01') & (df['txn_date'] < \"2024-08-01\")]\n",
    "    .drop(\"txn_date\", axis=1)\n",
    "    #.dropna()\n",
    ")\n",
    "X_train = df_train.drop(target_col, axis=1).to_numpy()\n",
    "y_train = df_train[target_col].to_numpy()\n",
    "\n",
    "# Holdout data\n",
    "df_test = (\n",
    "    df\n",
    "    [df['txn_date'] >= \"2024-08-01\"]\n",
    "    .drop(\"txn_date\", axis=1)\n",
    "    .fillna(0) # is_holiday_next_day is causing NULLs since the information of the next days at the edges of the dataframe are unavailable\n",
    ")\n",
    "\n",
    "X_test = df_test.drop(target_col, axis=1).to_numpy()\n",
    "y_test = df_test[target_col].to_numpy()\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the standard scaler\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply the scaling to the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature names\n",
    "feature_names = df_train.drop(target_col, axis=1).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7fa3b-5db8-4dff-849f-df1724112fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns with nulls in both train and test dataframes\n",
    "train_nulls = df_train.isnull().sum()[df_train.isnull().sum() > 0].sort_values(ascending=False)\n",
    "test_nulls = df_test.isnull().sum()[df_test.isnull().sum() > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Train Dataframe Null Columns:\")\n",
    "print(train_nulls)\n",
    "print(\"\\nTest Dataframe Null Columns:\")\n",
    "print(test_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ef6bf9-f7c6-47f6-a8b0-90b7b5bd9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the test set\n",
    "\n",
    "df_test[target_col].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f3ec0-601a-4bd4-bac9-d0465064847f",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e84e9-f2dc-4bfe-9900-c5f5e714ea34",
   "metadata": {},
   "source": [
    "### Evaluation helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd1c2a-ed2a-44aa-814a-b92568bfe9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def calculate_mase(y_true, y_pred, hours_per_week=168):\n",
    "    \"\"\"Computes the MASE.\"\"\"\n",
    "    # Convert inputs to numpy arrays\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true and y_pred must have the same length\")\n",
    "    \n",
    "    # Calculate errors for the actual predictions\n",
    "    prediction_errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Calculate naive forecast errors\n",
    "    if len(y_true) <= hours_per_week:\n",
    "        raise ValueError(f\"Length of y_true must be greater than hours_per_week ({hours_per_week})\")\n",
    "    \n",
    "    # Calculate week-over-week naive forecast errors\n",
    "    naive_errors = []\n",
    "    for i in range(hours_per_week, len(y_true)):\n",
    "        naive_forecast = y_true[i - hours_per_week]  # Use value from previous week\n",
    "        naive_errors.append(abs(y_true[i] - naive_forecast))\n",
    "    \n",
    "    naive_errors = np.array(naive_errors)\n",
    "    mean_naive_error = np.mean(naive_errors)\n",
    "    \n",
    "    if mean_naive_error == 0:\n",
    "        raise ValueError(\"Mean naive error is zero, MASE cannot be computed\")\n",
    "    \n",
    "    # Calculate MASE\n",
    "    # For seasonal naive, we only consider errors after the first week\n",
    "    mase = np.mean(prediction_errors[hours_per_week:]) / mean_naive_error\n",
    "    \n",
    "    return mase\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Computes the MAPE.\"\"\"\n",
    "    return np.mean(np.abs((np.array(y_true) - np.array(y_pred)) / np.array(y_true))) * 100\n",
    "\n",
    "# We use this helper function to evaluate model performance\n",
    "def evaluate(y_true, y_pred, printing=True):\n",
    "    \"\"\"\n",
    "    This helper function prints the performance metrics of the model\n",
    "    \"\"\"\n",
    "    # Evaluate the model using RMSE and MAE\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = calculate_mape(y_true, y_pred)\n",
    "    mase = calculate_mase(y_true, y_pred)\n",
    "\n",
    "    # Print the model metrics\n",
    "    if printing:\n",
    "        print(f'MAE: {mae}')\n",
    "        print(f'MSE: {mse}')\n",
    "        print(f'RMSE: {rmse}')\n",
    "        print(f'R2: {r2}')\n",
    "        print(f'MAPE: {mape}')\n",
    "        print(f'MASE: {mase}\\n')\n",
    "\n",
    "    return {'RMSE': rmse, 'MAPE': mape, 'MASE': mase, 'MAE': mae, 'MSE': mse,  'R2': r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ae203-9588-46d7-a9b2-5731fd2c6f14",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Use previous week's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee543d-1f53-49d6-95a4-c8ab2fc832be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_naive = pd.DataFrame(df[(df['txn_date'] >= '2024-07-25') & (df['txn_date'] <= '2024-08-31')][target_col])\n",
    "df_naive['forecast_d-1'] = df_naive[target_col].shift(24)\n",
    "df_naive['forecast_w-1'] = df_naive[target_col].shift(7 * 24)\n",
    "df_naive = df_naive[df['txn_date'] >= '2024-08-01'] # Filter Aug 2024 data\n",
    "display(df_naive.head())\n",
    "\n",
    "print(\"d-1\")\n",
    "_ = evaluate(df_naive[target_col], df_naive[\"forecast_d-1\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"W-1\")\n",
    "naive_eval_metrics_test = evaluate(df_naive[target_col], df_naive[\"forecast_w-1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9edddb-fc69-4041-99e6-431927f7fcfe",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bc21a-24df-4c41-bbad-0cbf8f929995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space for alpha\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e5, log=True)  # Log scale search for alpha\n",
    "    \n",
    "    # Ridge regression model with the suggested alpha value\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    \n",
    "    # Fit the model\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "    \n",
    "    return rmse  \n",
    "\n",
    "# Create the Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')  # Minimize RMSE\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True, n_jobs=-1)  # Number of trials to run\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Train the final model with the best alpha\n",
    "ridge_model = Ridge(alpha=best_params['alpha'])\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the final model\n",
    "print(\"Train\")\n",
    "ridge_eval_metrics_train = evaluate(y_train, ridge_model.predict(X_train_scaled))\n",
    "print(\"Test\")\n",
    "ridge_eval_metrics_test = evaluate(y_test, y_pred_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40d8ab-2f08-4d42-8c45-6516da453467",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9dbb3-da77-42ac-9f47-35c8897f42bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to tune\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    \n",
    "    # Initialize the model with the suggested hyperparameters\n",
    "    dt_model = DecisionTreeRegressor(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    dt_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model on the validation set (or test set if needed)\n",
    "    y_pred = dt_model.predict(X_test)\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return rmse  # Optuna will try to minimize this\n",
    "\n",
    "# Hyperparameter optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True, n_jobs=-1)\n",
    "\n",
    "# Get the best model\n",
    "best_params = study.best_params\n",
    "dt_model = DecisionTreeRegressor(\n",
    "    max_depth=best_params['max_depth'],\n",
    "    min_samples_split=best_params['min_samples_split'],\n",
    "    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Train\")\n",
    "dt_eval_metrics_train = evaluate(y_train, dt_model.predict(X_train))\n",
    "print(\"Test\")\n",
    "dt_eval_metrics_test = evaluate(y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb136574-2377-478f-84e0-bdc5935128b2",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e679899-bb62-4208-b1f7-b95358573d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the random forest regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=300, \n",
    "                                 max_depth=6, \n",
    "                                 n_jobs=-1, \n",
    "                                 random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Train\")\n",
    "rf_eval_metrics_train = evaluate(y_train, rf_model.predict(X_train))\n",
    "print(\"Test\")\n",
    "rf_eval_metrics_test = evaluate(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cd942-52ab-4e1b-a822-289c8030ecb4",
   "metadata": {},
   "source": [
    "### XGBoost (with hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061e141-f24f-4268-9ba1-bea1c133f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters for tuning\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': 300, # Keep fixed\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.5, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7), # keep this low enough to avoid overfitting\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 1e2, log=True), # L1 regularization\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 1e2, log=True), # L2 regularization\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # Initialize the XGBoost regressor with the suggested hyperparameters\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate root mean squared error as the objective to minimize\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return rmse\n",
    "    \n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=60, show_progress_bar=True)\n",
    "\n",
    "# Get the best hyperparameters found by Optuna\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters found: \", best_params)\n",
    "\n",
    "# Train the best model\n",
    "xgb_model = xgb.XGBRegressor(**best_params, n_estimators=300, n_jobs = -1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgboost = xgb_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Train\")\n",
    "xgb_eval_metrics_train = evaluate(y_train, xgb_model.predict(X_train))\n",
    "print(\"Test\")\n",
    "xgb_eval_metrics_test = evaluate(y_test, y_pred_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d1c542-f9b1-4db8-b9e7-8a6314917258",
   "metadata": {},
   "source": [
    "#### Feature Importances of the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcae7fd-ccbb-4690-8be3-301b61beed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impt = dict(zip(feature_names, xgb_model.feature_importances_))\n",
    "\n",
    "# Sort the dictionary by feature importance in descending order\n",
    "k = 10\n",
    "top_k_importance = dict(sorted(feature_impt.items(), \n",
    "                                key=lambda item: item[1], reverse=True)[:k])\n",
    "\n",
    "# Extract the feature names and importances for plotting\n",
    "features = list(top_k_importance.keys())[::-1]\n",
    "importances = list(top_k_importance.values())[::-1]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.barh(features, importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.savefig(rf\"../../data/images/{borough} - XGBoost model feature importances.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875335ea-7afc-429a-a3a4-43d76bf16a46",
   "metadata": {},
   "source": [
    "#### SHAP on XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e1befb-30e7-4e3d-9d43-a0f8a2d64d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize SHAP explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(xgb_model, n_jobs=-1)\n",
    "\n",
    "# Compute SHAP values for the dataset (assuming X_train is your input data)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Plot summary of SHAP values (feature importances)\n",
    "shap.summary_plot(shap_values, X_train, feature_names=feature_names, show=False, max_display=10)\n",
    "\n",
    "# Save the plot as a JPEG file\n",
    "plt.savefig(rf\"../../data/images/{borough} - XGBoost model SHAP beeswarm plot.jpeg\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1eae74-f551-43c5-8a3d-d1f8e3115071",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9080b12-b82e-4bb9-989e-cbae47b04025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Fit the regression model\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(200, 150, 120, 90, 70),\n",
    "                        activation=\"relu\",\n",
    "                        solver=\"adam\", \n",
    "                        learning_rate=\"adaptive\", # Changing learning rate\n",
    "                        learning_rate_init=0.1, # Initial learning rate\n",
    "                        momentum=0.9, # Avoid local minima\n",
    "                        alpha=10, # Regularization\n",
    "                        max_iter=500,\n",
    "                        tol=1e-7,\n",
    "                        random_state=42, \n",
    "                        verbose=True,\n",
    "                        shuffle=False # NOTE THAT THIS IS TIME SERIES DATA NO SHUFFLING ALLOWED\n",
    "                       )\n",
    "nn_model.fit(X_train_scaled, y_train) # Use scaled data\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn_model.predict(X_test_scaled) # Use scaled data\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Train\")\n",
    "nn_eval_metrics_train = evaluate(y_train, nn_model.predict(X_train_scaled))\n",
    "print(\"Test\")\n",
    "nn_eval_metrics_test = evaluate(y_test, y_pred_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75affd8-1d28-4ccc-a0b1-402a13487b14",
   "metadata": {},
   "source": [
    "### FB Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b22c1-6e74-4737-b132-8d0d28d9d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Prophet has specific formatting\n",
    "df_prophet = df.copy()\n",
    "df_prophet = df_prophet.reset_index()\n",
    "df_prophet['timestamp_hour'] = pd.to_datetime(df_prophet['timestamp_hour'])\n",
    "df_prophet = df_prophet.sort_values(by='timestamp_hour')\n",
    "df_prophet = df_prophet.rename(columns = {'num_txns_All': 'y', 'timestamp_hour': 'ds'})\n",
    "df_prophet = df_prophet[['ds', 'y', 'rolling_avg_w1-w4_lags']]\n",
    "\n",
    "# Train test split\n",
    "df_hist_prophet = df_prophet[df_prophet['ds'] < '2024-08-01'].dropna()\n",
    "df_test_prophet = df_prophet[df_prophet['ds']>= '2024-08-01']\n",
    "\n",
    "# Best prophet model\n",
    "best_prophet = joblib.load(f'../../models/prophet/{borough}_Prophet.joblib')\n",
    "\n",
    "# New prophet model but with exogenous variables\n",
    "prophet_model = Prophet(changepoint_prior_scale=best_prophet.changepoint_prior_scale,\n",
    "                        seasonality_prior_scale=best_prophet.seasonality_prior_scale,\n",
    "                        holidays_prior_scale=best_prophet.holidays_prior_scale,\n",
    "                        seasonality_mode=best_prophet.seasonality_mode,\n",
    "                        yearly_seasonality=best_prophet.yearly_seasonality,\n",
    "                        weekly_seasonality=best_prophet.weekly_seasonality,\n",
    "                        daily_seasonality=best_prophet.daily_seasonality)\n",
    "\n",
    "# Add exogenous variables as regressors\n",
    "prophet_model.add_regressor('rolling_avg_w1-w4_lags')\n",
    "\n",
    "# Add custom hourly seasonality (you can adjust the period if needed)\n",
    "prophet_model.add_seasonality(name='hourly', period=24, fourier_order=8)\n",
    "\n",
    "# Fit the model\n",
    "prophet_model.fit(df_hist_prophet)\n",
    "\n",
    "# Forecast\n",
    "y_pred_prophet = prophet_model.predict(df_test_prophet)\n",
    "prophet_eval_metrics_test = evaluate(df_test_prophet['y'], y_pred_prophet['yhat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4c591-0f5d-4844-be8d-e769a50c2ce1",
   "metadata": {},
   "source": [
    "### FB Prophet + GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97dab0-1ecd-42b2-ac9a-53f60e81d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "# Specify the residuals from the Prophet model\n",
    "prophet_residuals = df_test_prophet['y'].values - y_pred_prophet['yhat'].values\n",
    "\n",
    "# Use Prophet to predict mu (trend/seasonality)\n",
    "predicted_mu = y_pred_prophet['yhat']\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest values for p and q\n",
    "    p = trial.suggest_int('p', 1, 30)  # GARCH order (p)\n",
    "    q = trial.suggest_int('q', 1, 30)  # ARCH order (q)\n",
    "    \n",
    "    # Fit the GARCH model with the suggested hyperparameters\n",
    "    garch_model = arch_model(prophet_residuals, vol='Garch', p=p, q=q, rescale=False)\n",
    "    garch_fit = garch_model.fit(disp='off')\n",
    "    \n",
    "    # Forecast residuals for the next 10 periods (horizon)\n",
    "    garch_forecast = garch_fit.forecast(horizon=10)\n",
    "    predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "    \n",
    "    # Combine predictions (yt = mu + et)\n",
    "    y_pred_garch = np.array(predicted_mu + predicted_et)\n",
    "    \n",
    "    # Evaluate the GARCH model (you need to define `evaluate` function)\n",
    "    garch_eval_metrics_test = evaluate(y_test, y_pred_garch, printing=False)\n",
    "    \n",
    "    # Return the evaluation metric, assuming lower is better (e.g., MASE, RMSE)\n",
    "    return garch_eval_metrics_test['RMSE']  # Change this depending on your evaluation metric\n",
    "\n",
    "# Create an Optuna study to optimize the hyperparameters\n",
    "study = optuna.create_study(direction='minimize')  # Minimize the evaluation metric (e.g., MAE)\n",
    "\n",
    "# Run the optimization for 50 trials\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True, n_jobs=-1)\n",
    "\n",
    "# Train the final Prophet+GARCH model\n",
    "# Best p and q for GARCH\n",
    "best_p = study.best_params['p']\n",
    "best_q = study.best_params['q']\n",
    "\n",
    "# Fit GARCH model to Prophet's residuals\n",
    "garch_model = arch_model(prophet_residuals, vol='Garch', p=best_p, q=best_q, rescale=False)\n",
    "garch_fit = garch_model.fit(disp='off')\n",
    "\n",
    "# Use GARCH to predict the residual (et)\n",
    "garch_forecast = garch_fit.forecast(horizon=10)\n",
    "predicted_et = garch_forecast.mean['h.01'].iloc[-1]\n",
    "\n",
    "# Combine predictions (yt = mu (from Prophet) + et) \n",
    "y_pred_garch = np.array(predicted_mu + predicted_et)\n",
    "\n",
    "# Evaluate the GARCH model\n",
    "garch_eval_metrics_test = evaluate(y_test, y_pred_garch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32f591-b810-4ef5-815e-1b6cd4221f56",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a2cb5-a948-42e8-9b35-eac088105993",
   "metadata": {},
   "source": [
    "### Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828fd25-dac0-490b-a208-18e4a4b75002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "num_hrs = 168 * 2 # plot only the last 2 weeks\n",
    "# Plotting the predictions\n",
    "ax.plot(df_test.index[-num_hrs:], df_test[target_col][-num_hrs:], label=\"Actual\", color='black', lw=1.5)\n",
    "ax.plot(df_test.index[-num_hrs:], df_naive[\"forecast_w-1\"][-num_hrs:], label=f\"Naive (Week-1) (RMSE={naive_eval_metrics_test['RMSE']:.2f}, MASE={naive_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_ridge[-num_hrs:], label=f\"Ridge Regression (RMSE={ridge_eval_metrics_test['RMSE']:.2f}, MASE={ridge_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_dt[-num_hrs:], label=f\"Decision Tree (RMSE={dt_eval_metrics_test['RMSE']:.2f}, MASE={dt_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_rf[-num_hrs:], label=f\"Random Forest (RMSE={rf_eval_metrics_test['RMSE']:.2f}, MASE={rf_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_xgboost[-num_hrs:], label=f\"XGBoost (RMSE={xgb_eval_metrics_test['RMSE']:.2f}, MASE={xgb_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_nn[-num_hrs:], label=f\"ANN (RMSE={nn_eval_metrics_test['RMSE']:.2f}, MASE={nn_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_prophet[\"yhat\"].to_numpy()[-num_hrs:], label=f\"Prophet (RMSE={prophet_eval_metrics_test['RMSE']:.2f}, MASE={prophet_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_garch[-num_hrs:], label=f\"Prophet+GARCH (RMSE={garch_eval_metrics_test['RMSE']:.2f}, MASE={garch_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "\n",
    "# Adding labels, title, and legend for better clarity\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of Transactions')\n",
    "ax.set_title(f'{borough} Transactions')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()\n",
    "plt.tick_params(axis='x', labelrotation=45)  # Rotate x-axis labels 45 degrees\n",
    "\n",
    "img_filename = rf\"../../data/results/{borough} - test set performance comparison.jpeg\"\n",
    "plt.savefig(img_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460df42d-82cf-43f3-adac-a5ecf4606a10",
   "metadata": {},
   "source": [
    "### XGBoost and Naive Model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66a529-f05e-4e5b-93d7-5e57b30d7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "num_hrs = 168 * 2 # plot only the last 2 weeks\n",
    "# Plotting the predictions\n",
    "ax.plot(df_test.index[-num_hrs:], df_test[target_col][-num_hrs:], label=\"Actual\", color='black', lw=1.5)\n",
    "ax.plot(df_test.index[-num_hrs:], df_naive[\"forecast_w-1\"][-num_hrs:], label=f\"Naive (Week-1) (RMSE={naive_eval_metrics_test['RMSE']:.2f}, MASE={naive_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "ax.plot(df_test.index[-num_hrs:], y_pred_xgboost[-num_hrs:], label=f\"XGBoost (RMSE={xgb_eval_metrics_test['RMSE']:.2f}, MASE={xgb_eval_metrics_test['MASE']:.2f})\", ls='--', lw=.8)\n",
    "\n",
    "# Adding labels, title, and legend for better clarity\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Number of Transactions')\n",
    "ax.set_title(f'{borough} Transactions')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()\n",
    "plt.tick_params(axis='x', labelrotation=45)  # Rotate x-axis labels 45 degrees\n",
    "\n",
    "img_filename = rf\"../../data/results/{borough} - test set performance comparison (XGBoost only).jpeg\"\n",
    "plt.savefig(img_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c0059a-39af-4a75-ac86-7b3db4f0dbf1",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8794634-0b61-4bdb-afcc-7bed0b6cd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping model names to their metrics\n",
    "models_dict = {\n",
    "    'Naive (W-1)': naive_eval_metrics_test,\n",
    "    'Ridge Regression': ridge_eval_metrics_test,\n",
    "    'Decision Tree': dt_eval_metrics_test,\n",
    "    'Random Forest': rf_eval_metrics_test,\n",
    "    'XGBoost': xgb_eval_metrics_test,\n",
    "    'Neural Network': nn_eval_metrics_test,\n",
    "    'Prophet': prophet_eval_metrics_test,\n",
    "    'Prophet + GARCH': garch_eval_metrics_test\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_metrics = pd.DataFrame.from_dict(models_dict, orient='index')\n",
    "df_metrics[\"borough\"] = borough\n",
    "df_metrics = df_metrics[[\"borough\", \"RMSE\", \"MAPE\", \"MASE\", \"MAE\"]].fillna(0)\n",
    "\n",
    "csv_filename = rf\"../../data/results/{borough} - model metrics.csv\"\n",
    "df_metrics.to_csv(csv_filename)\n",
    "df_metrics.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb5c20-40cd-4144-91fc-09e5a60bc7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for MAPE and MASE\n",
    "x = np.arange(len(df_metrics.index))  # the label locations\n",
    "\n",
    "# Subplot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "# Highlight only the best models\n",
    "mape_best_model = df_metrics[\"MAPE\"].sort_values().index[0]\n",
    "colors_mape = ['skyblue' if model != mape_best_model else '#A52A2A' for model in df_metrics.index]\n",
    "\n",
    "mase_best_model = df_metrics[\"MASE\"].sort_values().index[0]\n",
    "colors_mase = ['salmon' if model != mase_best_model else '#A52A2A' for model in df_metrics.index]\n",
    "\n",
    "rmse_best_model = df_metrics[\"RMSE\"].sort_values().index[0]\n",
    "colors_rmse = ['#E1BEE7' if model != rmse_best_model else '#A52A2A' for model in df_metrics.index]\n",
    "\n",
    "# Subplot 1: MAPE\n",
    "axes[0].bar(x, df_metrics['MAPE'], color=colors_mape, width=0.6)\n",
    "axes[0].set_title('MAPE', fontsize=14)\n",
    "axes[0].set_ylabel('MAPE', fontsize=12)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(df_metrics.index, rotation=45, fontsize=10)\n",
    "axes[0].bar_label(axes[0].containers[0], fmt='%.2f%%', padding=3)\n",
    "axes[0].set_ylim(0, df_metrics['MAPE'].max() + 2)\n",
    "\n",
    "# Subplot 2: MASE\n",
    "axes[1].bar(x, df_metrics['MASE'], color=colors_mase, width=0.6)\n",
    "axes[1].axhline(y=1, color='black', lw=.7)\n",
    "axes[1].set_title('MASE', fontsize=14)\n",
    "axes[1].set_ylabel('MASE', fontsize=12)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(df_metrics.index, rotation=45, fontsize=10)\n",
    "axes[1].bar_label(axes[1].containers[0], fmt='%.2f', padding=3)\n",
    "axes[1].set_ylim(0, df_metrics['MASE'].max() + .2)\n",
    "\n",
    "# Sublot 3: RMSE\n",
    "axes[2].bar(x, df_metrics['RMSE'], color=colors_rmse, width=0.6)\n",
    "axes[2].set_title('RMSE', fontsize=14)\n",
    "axes[2].set_ylabel('RMSE', fontsize=12)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(df_metrics.index, rotation=45, fontsize=10)\n",
    "axes[2].bar_label(axes[2].containers[0], fmt='%.1f', padding=3)\n",
    "axes[2].set_ylim(0, df_metrics['RMSE'].max() + 300)\n",
    "\n",
    "# Add a suptitle for the entire figure\n",
    "fig.suptitle(f'{borough} Model Performance Comparison', fontsize=16)\n",
    "\n",
    "# Adjust layout and show\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "img_filename = rf\"../../data/results/{borough} - metrics comparison.jpeg\"\n",
    "plt.savefig(img_filename, dpi=300, bbox_inches='tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd2133-df08-4352-b66a-c18323a51806",
   "metadata": {},
   "source": [
    "### ADF test and ACF+PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1d9f9-258f-4a02-8d49-2896a2a82899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# time_series = df[(df['txn_date'] >= '2023-01-01') & (df['txn_date'] <= '2024-12-01')][target_col].to_numpy()\n",
    "# result = adfuller(time_series)\n",
    "# print('ADF Statistic: %f' % result[0])\n",
    "# print('p-value:', result[1])\n",
    "# print('Critical Values:')\n",
    "# for key, value in result[4].items():\n",
    "#     print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Time series data\n",
    "time_series = df[(df['txn_date'] >= '2023-01-01') & (df['txn_date'] < \"2024-08-01\")][\"num_txns_All\"]\n",
    "\n",
    "# Max time lag \n",
    "lag = 200\n",
    "\n",
    "# Tick positions\n",
    "tick_positions = np.arange(0, lag + 1, 24)  # From 0 to max(x) in steps of 24\n",
    "tick_labels = [int(t) for t in tick_positions]  # Custom labels\n",
    "\n",
    "# Plot ACF and PACF\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 7))\n",
    "\n",
    "# Autocorrelation (ACF)\n",
    "plot_acf(\n",
    "    time_series,\n",
    "    ax=ax[0],\n",
    "    lags=lag,\n",
    "    vlines_kwargs={\"linewidth\": 0.5},  # Adjust line width\n",
    "    markersize=3  # Adjust marker size\n",
    ")\n",
    "ax[0].set_title(f\"{borough} ACF\")\n",
    "ax[0].set_xlabel(\"Hours\")\n",
    "ax[0].set_xticks(tick_positions)  # Set tick positions\n",
    "ax[0].set_xticklabels(tick_labels)  # Set custom labels\n",
    "ax[0].grid(True, which='both', linestyle='--', linewidth=0.5)  # Add grid\n",
    "\n",
    "# Partial Autocorrelation (PACF)\n",
    "plot_pacf(\n",
    "    time_series,\n",
    "    ax=ax[1],\n",
    "    lags=lag,\n",
    "    method='ywm',\n",
    "    vlines_kwargs={\"linewidth\": 0.5},  # Adjust line width\n",
    "    markersize=3  # Adjust marker size\n",
    ")\n",
    "ax[1].set_title(f\"{borough} PACF\")\n",
    "ax[1].set_xlabel(\"Hours\")\n",
    "ax[1].set_xticks(tick_positions)  # Set tick positions\n",
    "ax[1].set_xticklabels(tick_labels)  # Set custom labels\n",
    "ax[1].grid(True, which='both', linestyle='--', linewidth=0.5)  # Add grid\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(rf\"../../data/images/acf_pacf_{borough}.jpeg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f14df01-5eed-4b41-b810-01d7372ba751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
